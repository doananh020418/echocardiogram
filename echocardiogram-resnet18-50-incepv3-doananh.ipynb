{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:02.016077Z",
     "iopub.status.busy": "2021-11-03T09:01:02.015674Z",
     "iopub.status.idle": "2021-11-03T09:01:02.028366Z",
     "shell.execute_reply": "2021-11-03T09:01:02.027245Z",
     "shell.execute_reply.started": "2021-11-03T09:01:02.016012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import copy\n",
    "\n",
    "input_path = \"../input/data-chamber/DATA_CHAMBER_2021/\" \n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:02.030735Z",
     "iopub.status.busy": "2021-11-03T09:01:02.030031Z",
     "iopub.status.idle": "2021-11-03T09:01:04.260722Z",
     "shell.execute_reply": "2021-11-03T09:01:04.259858Z",
     "shell.execute_reply.started": "2021-11-03T09:01:02.030689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet161-17b70270.pth\t  resnet50-19c8e357.pth\r\n",
      "inception_v3_google-1a9a5a14.pth  squeezenet1_0-a815701f.pth\r\n",
      "resnet18-5c106cde.pth\t\t  squeezenet1_1-f364aa15.pth\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import join, exists, expanduser\n",
    "\n",
    "cache_dir = expanduser(join('~', '.torch'))\n",
    "\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)\n",
    "    \n",
    "!cp ../input/pretrained-pytorch-models/* ~/.torch/models/\n",
    "!ls ~/.torch/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:04.264732Z",
     "iopub.status.busy": "2021-11-03T09:01:04.264451Z",
     "iopub.status.idle": "2021-11-03T09:01:04.270814Z",
     "shell.execute_reply": "2021-11-03T09:01:04.270103Z",
     "shell.execute_reply.started": "2021-11-03T09:01:04.264683Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:04.272260Z",
     "iopub.status.busy": "2021-11-03T09:01:04.271901Z",
     "iopub.status.idle": "2021-11-03T09:01:05.729546Z",
     "shell.execute_reply": "2021-11-03T09:01:05.728554Z",
     "shell.execute_reply.started": "2021-11-03T09:01:04.272223Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:05.732864Z",
     "iopub.status.busy": "2021-11-03T09:01:05.732134Z",
     "iopub.status.idle": "2021-11-03T09:01:06.454139Z",
     "shell.execute_reply": "2021-11-03T09:01:06.453342Z",
     "shell.execute_reply.started": "2021-11-03T09:01:05.732809Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract = False, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    if model_name == \"resnet50\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model('resnet50', 3, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:06.455926Z",
     "iopub.status.busy": "2021-11-03T09:01:06.455590Z",
     "iopub.status.idle": "2021-11-03T09:01:09.098111Z",
     "shell.execute_reply": "2021-11-03T09:01:09.097258Z",
     "shell.execute_reply.started": "2021-11-03T09:01:06.455879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7f3de4c3e4e0>, 'validation': <torch.utils.data.dataloader.DataLoader object at 0x7f3de4c3e4a8>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([   # Here we do not make data augmentations\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299), # Note that we want to use Inception v3, it requires this size of images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # We can simply use this parameter\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    ImageFolderWithPaths(input_path + 'train', data_transforms['train']),\n",
    "    'validation': \n",
    "    ImageFolderWithPaths(input_path + 'test', data_transforms['validation'])\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','validation']}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=8,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=8,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)  # for Kaggle\n",
    "}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# See some statistics\n",
    "print(dataloaders)\n",
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.099699Z",
     "iopub.status.busy": "2021-11-03T09:01:09.099399Z",
     "iopub.status.idle": "2021-11-03T09:01:09.491922Z",
     "shell.execute_reply": "2021-11-03T09:01:09.490895Z",
     "shell.execute_reply.started": "2021-11-03T09:01:09.099651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# def imshow(inp, title=None):\n",
    "#     \"\"\"Imshow for Tensor.\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes,_ = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. General functions to train and visualize\n",
    "\n",
    "Here we use a general function to train a model. It includes:\n",
    "\n",
    "* Scheduling the learning rate\n",
    "* Saving the best model\n",
    "\n",
    "We use [*torch.optim.lr_scheduler*](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate). It provides several methods to adjust the learning rate based on the number of epochs. Our function parameter `scheduler` is an object from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.493676Z",
     "iopub.status.busy": "2021-11-03T09:01:09.493360Z",
     "iopub.status.idle": "2021-11-03T09:01:09.520486Z",
     "shell.execute_reply": "2021-11-03T09:01:09.519669Z",
     "shell.execute_reply.started": "2021-11-03T09:01:09.493627Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=2, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','validation']:\n",
    "            train_batches = len(dataloaders[phase])\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i,(inputs, labels,_) in enumerate(dataloaders[phase]):\n",
    "                print(\"\\rTraining batch {}/{}\".format(i+1, train_batches), end='', flush=True)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    # mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    # but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('/n{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model predictions\n",
    "\n",
    "A generic function to display predictions for a few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.528232Z",
     "iopub.status.busy": "2021-11-03T09:01:09.525675Z",
     "iopub.status.idle": "2021-11-03T09:01:09.539830Z",
     "shell.execute_reply": "2021-11-03T09:01:09.538985Z",
     "shell.execute_reply.started": "2021-11-03T09:01:09.528175Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels,_) in enumerate(dataloaders['validation']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer learning: feature extractor\n",
    "\n",
    "Here we use **Inception v3** as a fixed feature extractor.\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need to set `requires_grad == False` to freeze the parameters so that the gradients are not computed in `backward()`.\n",
    "\n",
    "### Inception v3\n",
    "\n",
    "Inception v3 was first described in [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567v1.pdf). This network is unique because it has two output layers when training. \n",
    "\n",
    "The second output is known as an auxiliary output and is contained in the AuxLogits part of the network. The primary output is a linear layer at the end of the network. \n",
    "\n",
    "Note, when testing we only consider the primary output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and evaluate\n",
    "\n",
    "We use [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) to schedule the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.547548Z",
     "iopub.status.busy": "2021-11-03T09:01:09.544668Z",
     "iopub.status.idle": "2021-11-03T09:01:09.558706Z",
     "shell.execute_reply": "2021-11-03T09:01:09.557830Z",
     "shell.execute_reply.started": "2021-11-03T09:01:09.547470Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, criterion, optimizer):\n",
    "    labels_input=list()\n",
    "    labels_output=list()\n",
    "    vid_id = list()\n",
    "    for phase in ['validation']:\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels, fname in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels_input= labels_input + labels.tolist()\n",
    "            for f in fname:\n",
    "                vid_id.append(f.split('/')[-1].split('.')[0].split('_')[0])\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            labels_output= labels_output + preds.tolist()\n",
    "    return labels_input,labels_output,vid_id\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.562844Z",
     "iopub.status.busy": "2021-11-03T09:01:09.562462Z",
     "iopub.status.idle": "2021-11-03T09:01:09.605126Z",
     "shell.execute_reply": "2021-11-03T09:01:09.604586Z",
     "shell.execute_reply.started": "2021-11-03T09:01:09.562799Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer_conv = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Decay LR by a factor of 0.1 every epoch\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T09:01:09.607646Z",
     "iopub.status.busy": "2021-11-03T09:01:09.607228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n",
      "Training batch 840/840/ntrain Loss: 0.1827 Acc: 0.9282\n",
      "Training batch 201/201/nvalidation Loss: 0.3289 Acc: 0.8768\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Training batch 840/840/ntrain Loss: 0.0214 Acc: 0.9952\n",
      "Training batch 201/201/nvalidation Loss: 0.2004 Acc: 0.9073\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Training batch 840/840/ntrain Loss: 0.0141 Acc: 0.9970\n",
      "Training batch 201/201/nvalidation Loss: 0.2052 Acc: 0.9079\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Training batch 840/840/ntrain Loss: 0.0119 Acc: 0.9979\n",
      "Training batch 201/201/nvalidation Loss: 0.1978 Acc: 0.9004\n",
      "\n",
      "Training complete in 8m 55s\n",
      "Best val Acc: 0.907903\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=4, is_inception=False) # As an example, only show the results of 2 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_model(model_ft)\n",
    "y_true,y_pred,vid_id = test_model(model_ft, criterion, optimizer_conv)\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079029247044181"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(zip(y_true,y_pred,vid_id)),columns =['y_true','y_pred','vid_id'])\n",
    "df.to_csv('df.csv',encoding='utf-8',index=False)\n",
    "\n",
    "vid_list = list(set(df['vid_id'].values))\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for vid in vid_list:\n",
    "    #print(vid)\n",
    "    tmp_df = df[df['vid_id']==vid]\n",
    "    #print(len(tmp_df))\n",
    "    vid_pred = tmp_df['y_pred'].mode().values[0]\n",
    "    vid_label = tmp_df['y_true'].mode().values[0]\n",
    "    y_true.append(vid_label)\n",
    "    y_pred.append(vid_pred)\n",
    "    #print(vid_label,\"\\n\",vid_pred)\n",
    "    \n",
    "    #print('vid: {} label: {} pred: {}'.format(vid,vid_label,vid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487179487179487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
